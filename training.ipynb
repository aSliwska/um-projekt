{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5398bf90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:02:00.548040Z",
     "start_time": "2025-06-10T21:01:59.685085Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd60541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:02:01.918306Z",
     "start_time": "2025-06-10T21:02:01.460514Z"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLO(\"model/yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a70e681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:02:07.476726Z",
     "start_time": "2025-06-10T21:02:04.295035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/20000 C:\\Users\\Adam\\PycharmProjects\\um-projekt\\data\\images\\test\\cabc30fc-e7726578.jpg: 384x640 1 person, 4 cars, 65.3ms\n"
     ]
    }
   ],
   "source": [
    "before_training_results = model.predict('data/images/test', stream=True)\n",
    "\n",
    "for r in before_training_results:\n",
    "    r.show()\n",
    "    break\n",
    "\n",
    "before_training_results=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9083b4a57960b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:02:12.007571Z",
     "start_time": "2025-06-10T21:02:11.989255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+rocm6.3\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495fdaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:27:04.739177Z",
     "start_time": "2025-06-10T21:02:13.806760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.153 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.152 ðŸš€ Python-3.13.3 torch-2.7.1+rocm6.3 CUDA:0 (AMD Radeon RX 7800 XT, 16368MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=23, bgr=0.0, box=0.0, cache=False, cfg=None, classes=None, close_mosaic=0, cls=2.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=bdd100k.yaml, degrees=0.0, deterministic=True, device=0, dfl=7.5, dnn=False, dropout=0.2, dynamic=False, embed=None, epochs=2, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=23, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=model/yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=ex3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/ex3, save_frames=False, save_json=False, save_period=1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=12, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    432622  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,591,790 parameters, 2,591,774 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.10.cv1.conv.weight'\n",
      "Freezing layer 'model.10.cv1.bn.weight'\n",
      "Freezing layer 'model.10.cv1.bn.bias'\n",
      "Freezing layer 'model.10.cv2.conv.weight'\n",
      "Freezing layer 'model.10.cv2.bn.weight'\n",
      "Freezing layer 'model.10.cv2.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n",
      "Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n",
      "Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n",
      "Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n",
      "Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n",
      "Freezing layer 'model.13.cv1.conv.weight'\n",
      "Freezing layer 'model.13.cv1.bn.weight'\n",
      "Freezing layer 'model.13.cv1.bn.bias'\n",
      "Freezing layer 'model.13.cv2.conv.weight'\n",
      "Freezing layer 'model.13.cv2.bn.weight'\n",
      "Freezing layer 'model.13.cv2.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.13.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.13.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.cv1.conv.weight'\n",
      "Freezing layer 'model.16.cv1.bn.weight'\n",
      "Freezing layer 'model.16.cv1.bn.bias'\n",
      "Freezing layer 'model.16.cv2.conv.weight'\n",
      "Freezing layer 'model.16.cv2.bn.weight'\n",
      "Freezing layer 'model.16.cv2.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.16.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.16.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.17.conv.weight'\n",
      "Freezing layer 'model.17.bn.weight'\n",
      "Freezing layer 'model.17.bn.bias'\n",
      "Freezing layer 'model.19.cv1.conv.weight'\n",
      "Freezing layer 'model.19.cv1.bn.weight'\n",
      "Freezing layer 'model.19.cv1.bn.bias'\n",
      "Freezing layer 'model.19.cv2.conv.weight'\n",
      "Freezing layer 'model.19.cv2.bn.weight'\n",
      "Freezing layer 'model.19.cv2.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.19.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.19.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.20.conv.weight'\n",
      "Freezing layer 'model.20.bn.weight'\n",
      "Freezing layer 'model.20.bn.bias'\n",
      "Freezing layer 'model.22.cv1.conv.weight'\n",
      "Freezing layer 'model.22.cv1.bn.weight'\n",
      "Freezing layer 'model.22.cv1.bn.bias'\n",
      "Freezing layer 'model.22.cv2.conv.weight'\n",
      "Freezing layer 'model.22.cv2.bn.weight'\n",
      "Freezing layer 'model.22.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.cv3.conv.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.weight'\n",
      "Freezing layer 'model.22.m.0.cv3.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.22.m.0.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 148.3Â±56.6 MB/s, size: 54.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/carmel/Desktop/studia/um/um-projekt/data/labels/train.cache... 69863 images, 147 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70000/70000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 20.3Â±11.3 MB/s, size: 41.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/carmel/Desktop/studia/um/um-projekt/data/labels/val... 10000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 3564.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/carmel/Desktop/studia/um/um-projekt/data/labels/val.cache\n",
      "Plotting labels to runs/detect/ex3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005390625), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 12 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/ex3\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2      2.46G          0      9.039      5.364        323        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3044/3044 [05:14<00:00,  9.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:31<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     186033       0.36      0.239       0.23      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      2.58G          0      6.142       5.31        307        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3044/3044 [05:02<00:00, 10.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:31<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     186033      0.483       0.25      0.248      0.138\n",
      "\n",
      "2 epochs completed in 0.190 hours.\n",
      "Optimizer stripped from runs/detect/ex3/weights/last.pt, 5.4MB\n",
      "Optimizer stripped from runs/detect/ex3/weights/best.pt, 5.4MB\n",
      "\n",
      "Validating runs/detect/ex3/weights/best.pt...\n",
      "Ultralytics 8.3.152 ðŸš€ Python-3.13.3 torch-2.7.1+rocm6.3 CUDA:0 (AMD Radeon RX 7800 XT, 16368MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,584,102 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:31<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     186033      0.485       0.25      0.248      0.138\n",
      "            pedestrian       3261      13426      0.447      0.405      0.386      0.186\n",
      "               bicycle        592       1039      0.377      0.202      0.199     0.0896\n",
      "                   car       9882     102922      0.523      0.556      0.564      0.334\n",
      "            motorcycle        346        460      0.289      0.104     0.0881     0.0355\n",
      "                 rider        527        658      0.473      0.199      0.202      0.104\n",
      "                   bus       1299       1660        0.4      0.345      0.324      0.251\n",
      "                 train         14         15          1          0          0          0\n",
      "                 truck       2734       4245      0.458      0.287      0.296      0.206\n",
      "          traffic sign       8211      34724      0.436      0.217      0.221      0.107\n",
      "         traffic light       5651      26884      0.447      0.185      0.199     0.0665\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/ex3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data=\"bdd100k.yaml\", name=\"ex3\", time=10., patience=10, save_period=1, cache=False, workers=12, \n",
    "                      imgsz=640, device=0, batch=23, plots=True, dropout=0.2, cls=2.5, dfl=7.5, box=0., pretrained=True,\n",
    "                      seed=42, optimizer='AdamW', lr0=0.01, lrf=0.01, momentum=0.9, weight_decay=0.0005, warmup_epochs=3,\n",
    "                      warmup_momentum=0.8, warmup_bias_lr=0.1, close_mosaic=0, freeze=23, resume=False, amp=True, exist_ok=True) # time=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82017aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276fb640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:27:19.665805Z",
     "start_time": "2025-06-10T21:27:18.869142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/20000 C:\\Users\\Adam\\PycharmProjects\\um-projekt\\data\\images\\test\\cabc30fc-e7726578.jpg: 384x640 1 person, 4 cars, 45.4ms\n"
     ]
    }
   ],
   "source": [
    "after_training_results = model.predict('data/images/test', stream=True)\n",
    "\n",
    "for r in after_training_results:\n",
    "    # r.boxes\n",
    "    # r.names\n",
    "    r.show()\n",
    "    break\n",
    "\n",
    "after_training_results=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f10e7e81b2d630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:29:35.715169Z",
     "start_time": "2025-06-10T21:27:36.967176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.152  Python-3.12.9 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 2060, 6144MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,584,102 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 381.7242.4 MB/s, size: 53.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Adam\\PycharmProjects\\um-projekt\\data\\labels\\val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 358/358 [01:27<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     186033      0.461      0.232      0.223      0.121\n",
      "            pedestrian       3261      13426      0.397      0.379      0.339      0.152\n",
      "                 rider        527        658      0.322     0.0456     0.0575     0.0225\n",
      "                   car       9882     102922      0.484       0.64      0.613      0.369\n",
      "                 truck       2734       4245      0.376      0.314      0.277      0.191\n",
      "                   bus       1299       1660      0.332      0.304      0.258      0.194\n",
      "                 train         14         15          1          0          0          0\n",
      "            motorcycle        346        460        0.3    0.00217       0.03     0.0132\n",
      "               bicycle        592       1039      0.505     0.0741      0.115     0.0488\n",
      "         traffic light       5651      26884       0.39      0.283      0.238     0.0752\n",
      "          traffic sign       8211      34724      0.505      0.282      0.301      0.149\n",
      "Speed: 0.2ms preprocess, 2.3ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Metric' object has no attribute 'map_per_class'. See valid attributes below.\n\n    Class for computing evaluation metrics for Ultralytics YOLO models.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m metrics = model.val(data=\u001b[33m\"\u001b[39m\u001b[33mbdd100k.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics.box.map_per_class)  \u001b[38;5;66;03m# mAP50 dla kaÅ¼dej klasy\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics.names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\ssn-projekt\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:394\u001b[39m, in \u001b[36mSimpleClass.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Provide a custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[32m    393\u001b[39m name = \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Metric' object has no attribute 'map_per_class'. See valid attributes below.\n\n    Class for computing evaluation metrics for Ultralytics YOLO models.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    "
     ]
    }
   ],
   "source": [
    "metrics = model.val(data=\"bdd100k.yaml\")\n",
    "\n",
    "print(metrics.box.map_per_class)  # mAP50 dla kaÅ¼dej klasy\n",
    "print(metrics.names)              # odpowiadajÄ…ce klasy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
